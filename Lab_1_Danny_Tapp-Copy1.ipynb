{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897882a4",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e68ac",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b8fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ff27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(6,6) # creates a random array of values between 0 and 1 in a 6x6 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2d7798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02619088, 0.36104403, 0.31129017, 0.42362089, 0.12894986,\n",
       "        0.22245678],\n",
       "       [0.46743626, 0.96315409, 0.23483198, 0.9035416 , 0.68133659,\n",
       "        0.98825399],\n",
       "       [0.64979274, 0.75853935, 0.61952911, 0.72217915, 0.30074408,\n",
       "        0.62849498],\n",
       "       [0.55869639, 0.5490333 , 0.42214116, 0.95030069, 0.23097357,\n",
       "        0.06298601],\n",
       "       [0.42719211, 0.05736204, 0.71913063, 0.09226325, 0.82600656,\n",
       "        0.76253464],\n",
       "       [0.1460198 , 0.80440129, 0.01655083, 0.37270676, 0.96737896,\n",
       "        0.70949499]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A # prints array to make sure it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb32ae",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1247758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64979274 0.75853935 0.61952911 0.72217915 0.30074408 0.62849498]\n"
     ]
    }
   ],
   "source": [
    "print(A[2]) # prints third row of the array since the first row would be print(A[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5a1fa",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df47d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02619088 0.31129017 0.12894986]\n",
      " [0.46743626 0.23483198 0.68133659]\n",
      " [0.64979274 0.61952911 0.30074408]\n",
      " [0.55869639 0.42214116 0.23097357]\n",
      " [0.42719211 0.71913063 0.82600656]\n",
      " [0.1460198  0.01655083 0.96737896]]\n"
     ]
    }
   ],
   "source": [
    "print(A[:,0::2]) # prints all of the values in the row portion with [:] \n",
    "                 # and then starts with the first column and goes by every \n",
    "                 # 2 to get all of the odd columns {,0::2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b43704",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1450ba9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_dictionary = {'even': 0, 'odd': 0} # creates a dictionary and initially asigns integer value 0 to even and odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe33a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(A.shape[0]): # i is now rows\n",
    "    for j in range(A.shape[1]): # j is now columns\n",
    "        if (i + j) % 2 == 0:\n",
    "            number_dictionary['even'] += A[i,j] # if the condition is met then it will add the values in the even coordinates\n",
    "        else:\n",
    "            number_dictionary['odd'] += A[i,j] # if the condition is not met then it will add the values in the odd coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3076a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'even': 10.312698867833316, 'odd': 7.753860646670889}\n"
     ]
    }
   ],
   "source": [
    "print(number_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a0931",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7b7440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrixMaker: # creates a class titled matrixMaker\n",
    "    def __init__(self,m,n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.A = np.random.rand(m,n) # random array with rows = m and columns = n\n",
    "    \n",
    "    def specialSum(self):\n",
    "        return np.sum(self.A[self.A > 0.5]) # sum of the values in the array that are greater than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4cd2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = matrixMaker(5,5) # this test creates a 5x5 matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf356938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.813530206936576\n"
     ]
    }
   ],
   "source": [
    "print(test.specialSum()) # this prints the specialSum of the test matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687bee9",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44091ad",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d66fc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79e9acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "...            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "...            'meeting', 'stating', 'siezing', 'itemization',\n",
    "...            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "...            'plotted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f3d70",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90ee2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa029bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer() # calls in PorterStemmer to then use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f2b69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [stem.stem(word) for word in plurals] # all of the stems in plurals assigned to stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d61d26ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de665adb",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0bc2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e84d7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Danny\n",
      "[nltk_data]     Tapp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6acf0d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Danny\n",
      "[nltk_data]     Tapp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4') # realized I had to download wordnet and omw-1.4 when I tried to use WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0716fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = [lemmatize.lemmatize(word) for word in plurals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "026b6617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fly', 'dy', 'mule', 'denied', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', 'siezing', 'itemization', 'sensational', 'traditional', 'reference', 'colonizer', 'plotted']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb252c",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ad22884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9898b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Wow, we can finally stop using the split function!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1cf6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Danny\n",
      "[nltk_data]     Tapp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be771c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text) # tokenized the sample text and assigned it to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f888fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', ',', 'we', 'can', 'finally', 'stop', 'using', 'the', 'split', 'function', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfcb0c",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009404a",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b18ae07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nemo.txt\", \"r\") as nemo_script: # opened up the text file (which was in the same folder as this notebook)\n",
    "    data = nemo_script.readlines()\n",
    "    \n",
    "total_words = 0 # assigned it to 0 to begin with\n",
    "    \n",
    "for line in data:\n",
    "    words = line.split() # this will get each word\n",
    "    total_words += len(words) # this will add up the words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3e404",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ec50234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263\n"
     ]
    }
   ],
   "source": [
    "print(total_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafacf11",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ec24ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLIN: 3357 words\n",
      "DORY: 1816 words\n",
      "GILL: 782 words\n",
      "NEMO: 661 words\n",
      "NIGEL: 430 words\n",
      "DENTIST: 324 words\n",
      "MR. RAY: 321 words\n",
      "PEACH: 283 words\n",
      "BRUCE: 279 words\n",
      "CRUSH: 272 words\n",
      "BLOAT: 251 words\n",
      "GURGLE: 194 words\n",
      "MOONFISH LEADER: 188 words\n",
      "CHUM: 129 words\n",
      "CORAL: 128 words\n",
      "DEB: 122 words\n",
      "SQUIRT: 92 words\n",
      "ANCHOR: 68 words\n",
      "BOB: 57 words\n",
      "DARLA: 52 words\n",
      "TAD: 51 words\n",
      "PELICAN: 48 words\n",
      "CRAB 2: 48 words\n",
      "PEARL: 47 words\n",
      "BUBBLES: 47 words\n",
      "BLOAT/BUBBLES/GURGLE: 47 words\n",
      "GULLS: 47 words\n",
      "BILL: 42 words\n",
      "SHELDON: 33 words\n",
      "LOBSTER: 31 words\n",
      "BRUCE/ANCHOR/CHUM: 30 words\n",
      "FISH: 27 words\n",
      "JACQUES: 26 words\n",
      "SWORDFISH: 24 words\n",
      "CRAB 1: 22 words\n",
      "TURTLE KID: 18 words\n",
      "FEMALE BIRD: 18 words\n",
      "ALL: 18 words\n",
      "CRAB 1/CRAB 2: 18 words\n",
      "SMALL FISH: 17 words\n",
      "MALE BIRD 1: 17 words\n",
      "MR. JOHANSSEN: 16 words\n",
      "DOLPHIN: 16 words\n",
      "CRAB: 16 words\n",
      "BARBARA: 15 words\n",
      "PATIENT: 15 words\n",
      "MALE BIRD 2: 12 words\n",
      "MARLIN/DORY: 10 words\n",
      "CRAB KID: 9 words\n",
      "TED: 9 words\n",
      "BOB/TED: 9 words\n",
      "KIDS: 8 words\n",
      "FISH KID: 8 words\n",
      "TURTLE KIDS: 8 words\n",
      "TURTLE KID 4: 8 words\n",
      "MALE BIRD 3: 8 words\n",
      "BOB/TED/BILL: 7 words\n",
      "FISH MOM: 6 words\n",
      "AQUASCUM: 6 words\n",
      "TURTLE KID 6: 5 words\n",
      "BOTH: 4 words\n",
      "TURTLE KID 3: 4 words\n",
      "TURTLE KID 5: 4 words\n",
      "GULL: 4 words\n",
      "DIVER: 3 words\n",
      "ANCHOR/CHUM: 3 words\n",
      "MOONFISH TED: 3 words\n",
      "TURTLE KID 1: 3 words\n",
      "TURTLE KID 2: 3 words\n",
      "BIG FISH: 3 words\n",
      "KRILL: 3 words\n",
      "PELICANS: 3 words\n",
      "FISH KIDS: 2 words\n",
      "JIMMY: 2 words\n",
      "GILL/NEMO: 1 words\n",
      "CRUSH/SQUIRT: 1 words\n",
      "DEB/BLOAT/GURGLE: 1 words\n",
      "NEMO/SQUIRT: 1 words\n",
      "FOOD!: 0 words\n",
      "AAAAAAAAAAHHH!!!: 0 words\n",
      "TA-DAA!: 0 words\n",
      "MOOOOO-WEEEEEEE-NEEEEED...: 0 words\n",
      "TOOOOOOO-FIIIIIIND...: 0 words\n",
      "HIS-SOOOOOOOOOOOON...: 0 words\n",
      "CAN-YOOOOOOOUUU-GIIIIIIIIIVE-USSSS-DIRECTIOOOOOOOONS-TOOOOOOOOO...: 0 words\n",
      "COOOME-BAAAAAAAAAAAAAACK!: 0 words\n",
      "WAAAAAAAAAAAAAAOOOOOOO!!! WAAAAAAAAAOOOOOO!!!: 0 words\n",
      "MOOOO..MOOOOOOOOOOOOOOO!!!: 0 words\n",
      "..AAAAAAAAAAT'SSS-GOOIIIIIIING..: 0 words\n",
      "..OOOOOOOOONNN?: 0 words\n",
      "AAAAAAAAAAAAAAAAAAHHH!!! AAAAAAAAAAAAAAAHHH!!!: 0 words\n",
      "THAAAAAAAAAAAAAAANK-YOOOOOOOOOOOOOUUUU-SIIIRRRRRRRRRRRRRRRR!: 0 words\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict # Needed a default dictionary instead of dict because it has default values and I won't get KeyError\n",
    "\n",
    "each_chr_word_count = defaultdict(int) # This will create a default value of 0 when we start counting a new character's word count\n",
    "\n",
    "with open(\"nemo.txt\",\"r\") as nemo_script:\n",
    "    data = nemo_script.readlines() # Opened up the text file\n",
    "    current_character = None # = None means that there's an absence of a value instead of 0 which is an integer,\n",
    "                             # value will be assigned later\n",
    "    for line in data:\n",
    "        line = line.strip() # removes leading and trailling whitespace characters from a string\n",
    "        if line.isupper(): # all character names were in full uppercase, so this checks for character names\n",
    "            current_character = line\n",
    "        elif current_character:\n",
    "            words = line.split() # separates the words in the lines\n",
    "            each_chr_word_count[current_character] += len(words) # adds the total words to the dictionary for the character\n",
    "            \n",
    "sorted_word_counts = sorted(each_chr_word_count.items(),key=lambda item: item[1], reverse=True) # ordered the word count in descending order\n",
    "\n",
    "for character, count in sorted_word_counts:\n",
    "    print(f\"{character}: {count} words\") # f string to make it easier to print character names and corresponding word counts\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c712477",
   "metadata": {},
   "source": [
    "#### Marlin spoke the most with a total of 3357 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
