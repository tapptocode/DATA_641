{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              tweet label\n",
      "0        1  The CDC currently reports 99031 deaths. In gen...  real\n",
      "1        2  States reported 1121 deaths a small rise from ...  real\n",
      "2        3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
      "3        4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
      "4        5  Populous states can generate large case counts...  real\n",
      "...    ...                                                ...   ...\n",
      "6415  6416  A tiger tested positive for COVID-19 please st...  fake\n",
      "6416  6417  ???Autopsies prove that COVID-19 is??ï¿½ a blood...  fake\n",
      "6417  6418  _A post claims a COVID-19 vaccine has already ...  fake\n",
      "6418  6419  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund  fake\n",
      "6419  6420  It has been 93 days since the last case of COV...  real\n",
      "\n",
      "[6420 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "trainlabels = pd.read_csv('TrainLabels.csv')\n",
    "print(trainlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import html\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_labels = trainlabels[trainlabels['label'] == 'real']\n",
    "fake_labels = trainlabels[trainlabels['label'] == 'fake']                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\809847916.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['tokenized_tweets'] = fake_labels['tweet'].apply(lambda x: word_tokenize(x))\n",
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\809847916.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['tokenized_tweets'] = fake_labels['tweet'].apply(lambda x: word_tokenize(x))\n"
     ]
    }
   ],
   "source": [
    "fake_labels['tokenized_tweets'] = fake_labels['tweet'].apply(lambda x: word_tokenize(x))\n",
    "fake_labels['tokenized_tweets'] = fake_labels['tweet'].apply(lambda x: word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Unique Words Count: 14859\n",
      "Fake Unique Words Count: 13258\n"
     ]
    }
   ],
   "source": [
    "real_unique_words = set([word for tweet in real_labels['tokenized_tweets'] for word in tweet]) \n",
    "fake_unique_words = set([word for tweet in fake_labels['tokenized_tweets'] for word in tweet]) \n",
    "real_unique_count = len(real_unique_words)\n",
    "fake_uniqe_count = len(fake_unique_words)\n",
    "print(\"Real Unique Words Count:\", real_unique_count)\n",
    "print(\"Fake Unique Words Count:\", fake_uniqe_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Words per Real Post: 38.49404761904762\n",
      "Average Words per Fake Post: 26.362418300653594\n"
     ]
    }
   ],
   "source": [
    "real_avg_words = real_labels['tokenized_tweets'].apply(len).mean()\n",
    "fake_avg_words = fake_labels['tokenized_tweets'].apply(len).mean()\n",
    "print(\"Average Words per Real Post:\",real_avg_words)\n",
    "print(\"Average Words per Fake Post:\",fake_avg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Characters per Real Post: 215.0613095238095\n",
      "Average Characters per Fake Post: 144.8875816993464\n"
     ]
    }
   ],
   "source": [
    "real_avg_chars = real_labels['tweet'].apply(len).mean()\n",
    "fake_avg_chars = fake_labels['tweet'].apply(len).mean()\n",
    "print(\"Average Characters per Real Post:\",real_avg_chars)\n",
    "print(\"Average Characters per Fake Post:\",fake_avg_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake posts appear to have lower words per post and characters per post in comparison to their real counterparts. \n",
    "#### The total unique word count is also lower for fake tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.S+','',text)\n",
    "    text = re.sub(r'@\\w+','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_lemmatize(text):\n",
    "    text = html.unescape(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(word.lower()) for word in tokens\n",
    "        if word.lower() not in stop_words and word.isalpha()\n",
    "    ]\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\2118398705.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels.loc[:,'cleaned_tokenized_tweets'] = fake_labels['tweet'] \\\n"
     ]
    }
   ],
   "source": [
    "real_labels.loc[:,'cleaned_tokenized_tweets'] = real_labels['tweet'] \\\n",
    "    .apply(remove_urls) \\\n",
    "    .apply(xml_lemmatize)\n",
    "\n",
    "fake_labels.loc[:,'cleaned_tokenized_tweets'] = fake_labels['tweet'] \\\n",
    "    .apply(remove_urls) \\\n",
    "    .apply(xml_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels['flesch_reading_ease'] = real_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.flesch_reading_ease(' '.join(x)))\n",
    "real_labels['flesch_kincaid_grade'] = real_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.flesch_kincaid_grade(' '.join(x)))\n",
    "real_labels['gunning_fog'] = real_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.gunning_fog(' '.join(x)))\n",
    "real_labels['smog_index'] = real_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.smog_index(' '.join(x)))\n",
    "real_labels['syllable_count'] = real_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.syllable_count(' '.join(x)))\n",
    "real_labels['sentence_count'] = real_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.sentence_count(' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\4024435890.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['flesch_reading_ease'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.flesch_reading_ease(' '.join(x)))\n",
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\4024435890.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['flesch_kincaid_grade'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.flesch_kincaid_grade(' '.join(x)))\n",
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\4024435890.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['gunning_fog'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.gunning_fog(' '.join(x)))\n",
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\4024435890.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['smog_index'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.smog_index(' '.join(x)))\n",
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\4024435890.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['syllable_count'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.syllable_count(' '.join(x)))\n",
      "C:\\Users\\Danny Tapp\\AppData\\Local\\Temp\\ipykernel_15736\\4024435890.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_labels['sentence_count'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.sentence_count(' '.join(x)))\n"
     ]
    }
   ],
   "source": [
    "fake_labels['flesch_reading_ease'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.flesch_reading_ease(' '.join(x)))\n",
    "fake_labels['flesch_kincaid_grade'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.flesch_kincaid_grade(' '.join(x)))\n",
    "fake_labels['gunning_fog'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.gunning_fog(' '.join(x)))\n",
    "fake_labels['smog_index'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.smog_index(' '.join(x)))\n",
    "fake_labels['syllable_count'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.syllable_count(' '.join(x)))\n",
    "fake_labels['sentence_count'] = fake_labels.loc[:,'cleaned_tokenized_tweets'].apply(lambda x: textstat.sentence_count(' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = pd.concat([real_labels,fake_labels],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       flesch_reading_ease  flesch_kincaid_grade  gunning_fog  smog_index  \\\n",
      "label                                                                       \n",
      "fake             22.157605             13.191307    15.481516         0.0   \n",
      "real             20.596833             14.191488    15.373884         0.0   \n",
      "\n",
      "       syllable_count  sentence_count  \n",
      "label                                  \n",
      "fake        24.454902             1.0  \n",
      "real        30.869048             1.0  \n"
     ]
    }
   ],
   "source": [
    "avg_read_scores = all_labels.groupby('label')[['flesch_reading_ease','flesch_kincaid_grade','gunning_fog','smog_index','syllable_count','sentence_count']].mean()\n",
    "print(avg_read_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The flesch kincaid grade shows that real posts are averaging a higher grade level than fake posts. The flesch reading ease has real posts slightly more confusing to read than fake posts, but that may be because fake posts are trying to be as accessible as possible. Real posts average more syllables than fake posts, meaning they may use more complex words in their tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
